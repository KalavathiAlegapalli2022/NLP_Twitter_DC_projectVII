# -*- coding: utf-8 -*-
"""Tweet_Recognition_NLP_PVII.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1iMj1dAajaXkKMjX5amBP4naNIftadRiV

# NLP Project for Disaster Tweet Classification

# Installing Important libraries:
"""

!pip install nlp

!pip install wordcloud

!pip install tensorflow

"""# Importing Important Libraries:"""

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline

import tensorflow as tf
import pandas as pd
import numpy as np
import seaborn as sns
import nlp
sns.set()
import matplotlib.pyplot as plt
from matplotlib.pyplot import figure
import matplotlib.patches as mpatches
import string
import re
from string import punctuation
import nltk
from nltk.tokenize import word_tokenize, sent_tokenize, TweetTokenizer, WordPunctTokenizer, RegexpTokenizer
from nltk.corpus import stopwords, wordnet
from wordcloud import WordCloud
from collections import Counter
import warnings
warnings.filterwarnings('ignore')

"""# Task 1: Data Exploration and Preparation"""

# Load the given dataset "twitter_disaster.csv":
df = pd.read_csv(r"/content/twitter_disaster.csv")

df

""" # Features of the dataset:

 **id :** A unique identifier corresponding to the tweet

**keyword :** A highlighting word from the tweet

**location :** The location from where the tweet is sent

**text :** The textual content of the tweet

**target :** A binary variable, which is 0
 if the tweet does not indicate a real disaster and 1
 if it does

 ### Note that the features keyword and location may be blank for many tweets.
"""

df.shape

# Dataset Structure
df.info()

print(df.keys())

df['target'].value_counts()

"""# Target : A binary variable, which is 0 if the tweet does not indicate a real disaster and 1 if it does

"""

df.duplicated().sum()

print(df.isnull().sum())

"""# There are 61 Keyword & 2533 location  Missing values

# Handling Missing Values:
"""

missing_cols = ['keyword', 'location']
for df in [df]:
    for col in ['keyword', 'location']:
        df[col] = df[col].fillna(f'no_{col}')

df.isnull().sum()

"""# Missing values in Keywords and Location are imputed by Pre-fixing "NO" to there index"""

df.describe()

df['target'].value_counts()

"""##  Visualize the distribution of classes (disaster vs. non-disaster tweets):"""

#Using Bar Plot:
plt.figure(figsize=(6, 4))
sns.countplot(x='target', data=df, palette='Set2')
plt.title('Distribution of Disaster vs Non-Disaster Tweets')
plt.xlabel('Class', color='red')
plt.ylabel('Count',color='red')
plt.xticks([0, 1], ['Non-Disaster = 0','Disaster = 1'])
plt.show()

"""# **"Distribution of Disaster vs Non-Disaster Tweets"**:

###  Dataset Composition
- **Class 0 (Non-Disaster Tweets)**: Over 4,000 tweets fall into this category.
- **Class 1 (Disaster Tweets)**: Slightly above 3,000 tweets are classified as disaster-related.
- **Imbalance**: The dataset is **moderately imbalanced**, with more non-disaster tweets than disaster tweets. This could influence model performance if not addressed.

## Analyze the frequency of keywords and phrases associated with disaster tweets:
"""

# Splitting the training data by target
disaster_tweets = df[df['target'] == 1]['text']
non_disaster_tweets = df[df['target'] ==0]['text']

# Class frequencies
print("Number of training tweets not indicating real disasters: {}".format(len(disaster_tweets)))
print("Number of training tweets indicating real disasters: {}".format(len(non_disaster_tweets)))

# Function to produce donutplot
def donutplot(value, label, color, title):
    fig = plt.figure(figsize = (10, 8))
    ax = fig.add_axes([0.0, 0.1, 1.0, 0.5], aspect = 1)
    pie = ax.pie(value, colors = color, autopct = "%1.1f%%", startangle = 90)
    centre_circle = plt.Circle((0, 0), 0.8, fc = "white")
    fig = plt.gcf()
    fig.gca().add_artist(centre_circle)
    fig.suptitle(title, y = 0.65, fontsize = 12)
    plt.legend(pie[0], label, loc = "upper left")
    plt.show()

# Visualization of class frequencies
target_frequency = np.array([len(non_disaster_tweets), len(disaster_tweets)])
target_label = ["Not disaster tweets", "Disaster tweets"]
target_color = ["green", "red"]
donutplot(value = target_frequency, label = target_label, color = target_color, title = "Frequency comparison of non-disaster tweets and disaster tweets")

"""
###  Class Distribution Overview
- **Non-Disaster Tweets (Green)**: Represent **57.0%** of the dataset.
- **Disaster Tweets (Red)**: Make up **43.0%** of the dataset.
- The distribution is **fairly balanced**, though non-disaster tweets are slightly more prevalent.

###  Modeling Implications
- **Mild Class Imbalance**: While not extreme, the imbalance could still affect model sensitivity toward disaster tweets.
- **Balanced Evaluation Metrics**: Metrics like **F1-score** and **ROC-AUC** will be more informative than accuracy alone."""

# Plot for Keywords in disaster tweets
plt.figure(figsize=(6,4))
sns.countplot(y=df.keyword, order = df.keyword.value_counts().iloc[:20].index)
plt.title('Top 20 keywords')
plt.xlabel("Count")
plt.ylabel("Keyword")
plt.show()

"""###  Keyword Frequency Analysis
- **Most Frequent Keyword**: `"no_keyword"` appears most often, suggesting many tweets may lack specific disaster-related terms.
- **High-Frequency Terms**: Keywords like `"fatalities"`, `"deluge"`, and `"damage"` are among the top, indicating common themes in disaster-related tweets.
- **Moderate Occurrence**: Keywords such as `"evacuate"`, `"outbreak"`, `"fear"`, and `"siren"` show moderate frequency, reflecting emergency response and public sentiment.

###  Semantic Relevance
- Many keywords (e.g., `"armageddon"`, `"hellfire"`, `"twister"`, `"wreckage"`) evoke strong imagery and are likely associated with high-impact events.
- The presence of emotionally charged or dramatic terms suggests that tweets may often use **intense language** during disasters.

###  NLP & Modeling Implications
- These keywords can serve as **features for classification models**, helping distinguish disaster-related tweets.
- The dominance of `"no_keyword"` may require special handling—either filtering or treating it as a separate category to avoid noise.


"""

# Visualizing the text length
df['text_length'] = df['text'].apply(len)
plt.figure(figsize=(10, 6))
sns.histplot(data=df, x='text_length', hue='target', bins= 50, kde=True)
plt.title('Distribution of Tweet Length by Class')
plt.xlabel('Tweet Length')
plt.ylabel('count')
plt.show()

"""## histogram showing tweet length distribution by class:

- Both disaster (class 1) and non-disaster (class 0) tweets peak near the 140-character mark, indicating users often use the full tweet length.
- Non-disaster tweets are more frequent across most lengths.
- The similar density curves suggest tweet length may be a useful but not strongly discriminative feature.
- The trend highlights potential for length-based features in classification models.

# Task: Data Preparation
"""

df['text'].head()

"""# Clean the text data by removing special characters, URLs, and punctuation marks."""

import re

def clean_text(text):

    # Remove URLs
    text = re.sub(r'http[s]?://\S+', '', text)

    # Remove special characters and punctuation marks
    text = re.sub(r'[^\w\s]', '', text)

    # Optionally, you can remove extra spaces
    text = re.sub(r'\s+', ' ', text).strip()

    return text

#Cleaning the text
df['cleaned_text'] = df['text'].apply(clean_text)

#Convert into lowercase
df['cleaned_text'] = df['cleaned_text'].str.lower()

df['cleaned_text']

from wordcloud import WordCloud
import matplotlib.pyplot as plt

df['cleaned_text'] = df['text'].apply(clean_text)

def generate_word_cloud(dataframe, column_name, title=None):
    # Combine all text in the specified column into one string
    text = " ".join(dataframe[column_name].astype(str))

    # Create the word cloud object
    wordcloud = WordCloud(width=800, height=400, background_color="white").generate(text)

    # Display the word cloud
    plt.figure(figsize=(10, 6))
    plt.imshow(wordcloud, interpolation="bilinear")
    plt.axis("off")  # Turn off axis

    # Add title if provided
    if title:
        plt.title(title, fontsize=16)

    plt.show()

# Run the function on the 'cleaned_text' column
generate_word_cloud(df, 'cleaned_text', title="Most Common Words in Disaster Tweets")

"""#  Tokenize the text into individual words or tokens"""

# For tokenizing text
nltk.download('punkt')
# For the punkt tokenizer model used by word_tokenize
nltk.download('punkt_tab')

# For stopwords
nltk.download('stopwords')

# Extract Categorical and Numerical features from tweets:

# Calculate tweet length feature
df['word_length'] = df['text'].apply(lambda x: len(x.split()))

# Calculate Special character feature and Punctuation feature
df['mention_count'] = df['cleaned_text'].str.count(r'@\w+')
df['hashtag_count'] = df['cleaned_text'].str.count(r'#\w+')
df['question_count'] = df['cleaned_text'].str.count(r'\?')
df['exclamation_count'] = df['cleaned_text'].str.count(r'!')

df['tokens'] = df['cleaned_text'].apply(word_tokenize)

df

"""# # Drop specified columns"""

df.drop(columns=['text', 'mention_count', 'hashtag_count', 'question_count', 'exclamation_count'], inplace=True)

# Display the updated DataFrame
print(df.head())

from IPython.display import FileLink

df.to_csv('preprocessed_data.csv', index=False)
FileLink('preprocessed_data.csv')

"""# Load preprocessed data:"""

df = pd.read_csv(r'/content/preprocessed_data.csv')

df

df.shape

df.info()

"""# Task 2: Feature Engineering and Model Selection

# Extract relevant features from the text data, such as word frequencies, TF-IDF scores, and sentiment analysis.

#  Basic Textual Features
## Word Frequencies (Bag of Words):
"""

from sklearn.feature_extraction.text import CountVectorizer

count_vectorizer = CountVectorizer()
X_bow = count_vectorizer.fit_transform(df['cleaned_text'])

X_bow

"""### TF-IDF (Term Frequency-Inverse Document Frequency):"""

from sklearn.feature_extraction.text import TfidfVectorizer

tfidf_vectorizer = TfidfVectorizer(max_features=5000)
X_tfidf = tfidf_vectorizer.fit_transform(df['cleaned_text'])

X_tfidf

"""##  BERT or Other Transformer Embeddings:"""

from transformers import BertTokenizer, BertModel
import torch
import numpy as np
from tqdm import tqdm

# Initialize tokenizer and model
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
model = BertModel.from_pretrained('bert-base-uncased')

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model.to(device)
model.eval()

def get_bert_embeddings_batch(texts, batch_size=32, max_length=512):
    """
    Get BERT embeddings for a list of texts using batch processing
    """
    embeddings = []

    # Process in batches
    for i in tqdm(range(0, len(texts), batch_size), desc="Processing batches"):
        batch_texts = texts[i:i+batch_size]

        # Tokenize batch
        inputs = tokenizer(
            batch_texts,
            return_tensors='pt',
            truncation=True,
            padding=True,
            max_length=max_length
        )

        # Move inputs to device
        inputs = {k: v.to(device) for k, v in inputs.items()}

        # Get embeddings without computing gradients
        with torch.no_grad():
            outputs = model(**inputs)
            # Mean pooling over sequence length
            batch_embeddings = outputs.last_hidden_state.mean(dim=1)

        # Move back to CPU and convert to numpy
        batch_embeddings = batch_embeddings.cpu().numpy()
        embeddings.extend(batch_embeddings)

    return np.array(embeddings)


df['bert_embedding'] = list(get_bert_embeddings_batch(df['cleaned_text'].tolist()))

# Alternative: If you want to keep the original function structure but optimize it
def get_bert_embedding_optimized(text, max_length=512):
    """Optimized single text embedding function"""
    inputs = tokenizer(
        text,
        return_tensors='pt',
        truncation=True,
        padding=True,
        max_length=max_length
    )
    inputs = {k: v.to(device) for k, v in inputs.items()}

    with torch.no_grad():
        outputs = model(**inputs)
        embedding = outputs.last_hidden_state.mean(dim=1).cpu().numpy()

    return embedding[0]  # Return 1D array instead of 2D

torch.cuda.empty_cache() if torch.cuda.is_available() else None

"""##  Sentiment Analysis:"""

from nltk.sentiment.vader import SentimentIntensityAnalyzer
import nltk
nltk.download('vader_lexicon')

sia = SentimentIntensityAnalyzer()
df['sentiment'] = df['cleaned_text'].apply(lambda x: sia.polarity_scores(x)['compound'])

"""# Experiment with additional features like tweet length, presence of hashtags, or user mentions"""

# Function to calculate additional features
def extract_additional_features(text):
    num_hashtags = len(re.findall(r"#\w+", text))
    num_mentions = len(re.findall(r"@\w+", text))
    return pd.Series([num_hashtags, num_mentions], index=['num_hashtags', 'num_mentions'])

# Apply function to both training and test sets
additional_future = df['cleaned_text'].apply(extract_additional_features)
additional_future

from scipy.sparse import hstack
import numpy as np

# Combine BoW/TF-IDF with other features
X_combined = hstack([X_tfidf,additional_future, np.array(df[['text_length','sentiment']])])

X_combined

"""## Split the dataset"""

from sklearn.model_selection import train_test_split

X = X_combined
y = df['target']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

"""##  Train Models and Evaluate Performance"""

from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.neural_network import MLPClassifier
from sklearn.model_selection import cross_val_score
from sklearn.metrics import accuracy_score, precision_score, recall_score, classification_report

"""# Logistic Regression Classification"""

from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(df['cleaned_text'], df['target'], test_size=0.2, random_state=42)

# Vectorize the text data using TF-IDF
vectorizer = TfidfVectorizer(max_features=5000, min_df=3, max_df=0.9)
X_train_tfidf = vectorizer.fit_transform(X_train)
X_test_tfidf = vectorizer.transform(X_test)

# Logistic Regression
logreg_model = LogisticRegression(C=0.5, penalty='l2', solver='liblinear')
logreg_model.fit(X_train_tfidf, y_train)

# Predictions
logreg_pred = logreg_model.predict(X_test_tfidf)

# Accuracy scores
train_accuracy = logreg_model.score(X_train_tfidf, y_train)
test_accuracy = accuracy_score(y_test, logreg_pred)

# Evaluation
print(" Logistic Regression Classification Report:")
print(classification_report(y_test, logreg_pred))

print(f" Train Accuracy: {train_accuracy:.3f}")
print(f" Test Accuracy : {test_accuracy:.3f}")

# Logistic regression model
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import f1_score
from sklearn.metrics import accuracy_score


C_vals = [10**-4,10**-3,10**-2,10**-1,10**0,10**1,10**2,10**3,10**4]

for num in C_vals:
    clf = LogisticRegression(C=num, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=5050,
                   multi_class='auto', n_jobs=-1, penalty='l2', random_state=42,
                   solver='lbfgs', tol=0.0001, verbose=0, warm_start=False)

    clf.fit(X_train_tfidf, y_train)
    predicted = clf.predict(X_test_tfidf)
    logistic_f1 = f1_score(y_test, predicted)
    print(f"F1 score = {logistic_f1}")
    print("C value = {}".format(num))
    print("Train Accuracy = {}%".format(clf.score(X_train_tfidf, y_train)*100))
    print("Test Accuracy = {}%".format(clf.score(X_test_tfidf, y_test)*100))
    print(70*"=")

"""#  Visualize the performance metrics using confusion matrices, ROC curves, and precision-recall curves"""

import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix, roc_curve, precision_recall_curve, auc

# Function to plot confusion matrix
def plot_confusion_matrix(y_true, y_pred, classes, title='Confusion Matrix', cmap=plt.cm.Blues):
    cm = confusion_matrix(y_true, y_pred)
    plt.figure(figsize=(8, 6))
    plt.imshow(cm, interpolation='nearest', cmap=cmap)
    plt.title(title)
    plt.colorbar()
    tick_marks = np.arange(len(classes))
    plt.xticks(tick_marks, classes, rotation=45)
    plt.yticks(tick_marks, classes)
    plt.xlabel('Predicted Label')
    plt.ylabel('True Label')
    plt.tight_layout()
    for i in range(len(classes)):
        for j in range(len(classes)):
            plt.text(j, i, cm[i, j], horizontalalignment='center', color='white' if cm[i, j] > cm.max() / 2 else 'black')
    plt.show()

# Function to plot ROC curve
def plot_roc_curve(y_true, y_score):
    fpr, tpr, _ = roc_curve(y_true, y_score)
    roc_auc = auc(fpr, tpr)
    plt.figure()
    plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)
    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('Receiver Operating Characteristic (ROC) Curve')
    plt.legend(loc="lower right")
    plt.show()

    # Function to plot precision-recall curve
def plot_precision_recall_curve(y_true, y_score):
    precision, recall, _ = precision_recall_curve(y_true, y_score)
    pr_auc = auc(recall, precision)
    plt.figure()
    plt.plot(recall, precision, color='blue', lw=2, label='Precision-Recall curve (area = %0.2f)' % pr_auc)
    plt.xlabel('Recall')
    plt.ylabel('Precision')
    plt.ylim([0.0, 1.05])
    plt.xlim([0.0, 1.0])
    plt.title('Precision-Recall Curve')
    plt.legend(loc="lower left")
    plt.show()

# Assuming nn_model is your trained neural network model
# Validate the selected model on the testing dataset
y_pred_probs = clf.predict(X_test_tfidf)
y_pred = (y_pred_probs > 0.5).astype(int)  # Apply threshold (0.5) to convert probabilities to binary predictions

# Plot confusion matrix
plot_confusion_matrix(y_test, y_pred, classes=['Non-Disaster', 'Disaster'])

# Plot ROC curve
plot_roc_curve(y_test, y_pred_probs)

# Plot precision-recall curve
plot_precision_recall_curve(y_test, y_pred_probs)

"""# Random Forest Classification"""

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, accuracy_score

# Random Forest
rf_model = RandomForestClassifier(random_state=42, n_estimators=100, n_jobs=-1)
rf_model.fit(X_train_tfidf, y_train)

# Prediction
rf_pred = rf_model.predict(X_test_tfidf)

# Accuracy scores
train_accuracy = rf_model.score(X_train_tfidf, y_train)
test_accuracy = accuracy_score(y_test, rf_pred)

# Evaluation
print(" Random Forest Classification Report:")
print(classification_report(y_test, rf_pred))

print(f" Train Accuracy (Random Forest): {train_accuracy:.3f}")
print(f" Test Accuracy  (Random Forest): {test_accuracy:.3f}")

print(classification_report(y_test, rf_pred))

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report

# List of values for the 'n_estimators' parameter
n_estimators_values = [50, 100, 150]

# Loop over each value of 'n_estimators' and train a RandomForestClassifier
for n_estimators in n_estimators_values:
    print(f"\n Training Random Forest with n_estimators = {n_estimators}")

    random_forest_model = RandomForestClassifier(n_estimators=n_estimators, random_state=42, n_jobs=-1)
    random_forest_model.fit(X_train_tfidf, y_train)

    # Evaluate Train Accuracy
    train_accuracy = random_forest_model.score(X_train_tfidf, y_train)

    # Evaluate Test Accuracy
    test_accuracy = random_forest_model.score(X_test_tfidf, y_test)

    # Make predictions
    predictions = random_forest_model.predict(X_test_tfidf)

    # Print accuracies
    print(f" Train Accuracy: {train_accuracy:.3f}")
    print(f" Test Accuracy : {test_accuracy:.3f}")

    # Classification Report
    print(" Classification Report:")
    print(classification_report(y_test, predictions))

"""# Visualize the performance metrics using confusion matrices, ROC curves, and precision-recall curves"""

import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix, roc_curve, precision_recall_curve, auc

# Function to plot confusion matrix
def plot_confusion_matrix(y_true, y_pred, classes, title='Confusion Matrix', cmap=plt.cm.Blues):
    cm = confusion_matrix(y_true, y_pred)
    plt.figure(figsize=(8, 6))
    plt.imshow(cm, interpolation='nearest', cmap=cmap)
    plt.title(title)
    plt.colorbar()
    tick_marks = np.arange(len(classes))
    plt.xticks(tick_marks, classes, rotation=45)
    plt.yticks(tick_marks, classes)
    plt.xlabel('Predicted Label')
    plt.ylabel('True Label')
    plt.tight_layout()
    for i in range(len(classes)):
        for j in range(len(classes)):
            plt.text(j, i, cm[i, j], horizontalalignment='center', color='white' if cm[i, j] > cm.max() / 2 else 'black')
    plt.show()

# Function to plot ROC curve
def plot_roc_curve(y_true, y_score):
    fpr, tpr, _ = roc_curve(y_true, y_score)
    roc_auc = auc(fpr, tpr)
    plt.figure()
    plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)
    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('Receiver Operating Characteristic (ROC) Curve')
    plt.legend(loc="lower right")
    plt.show()

    # Function to plot precision-recall curve
def plot_precision_recall_curve(y_true, y_score):
    precision, recall, _ = precision_recall_curve(y_true, y_score)
    pr_auc = auc(recall, precision)
    plt.figure()
    plt.plot(recall, precision, color='blue', lw=2, label='Precision-Recall curve (area = %0.2f)' % pr_auc)
    plt.xlabel('Recall')
    plt.ylabel('Precision')
    plt.ylim([0.0, 1.05])
    plt.xlim([0.0, 1.0])
    plt.title('Precision-Recall Curve')
    plt.legend(loc="lower left")
    plt.show()

# Validate the selected model on the testing dataset
y_pred_probs = random_forest_model.predict(X_test_tfidf)
y_pred = (y_pred_probs > 0.5).astype(int)  # Apply threshold (0.5) to convert probabilities to binary predictions

# Plot confusion matrix
plot_confusion_matrix(y_test, y_pred, classes=['Non-Disaster', 'Disaster'])

# Plot ROC curve
plot_roc_curve(y_test, y_pred_probs)

# Plot precision-recall curve
plot_precision_recall_curve(y_test, y_pred_probs)

"""# NLP model MLPClassifier"""

import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics import classification_report, accuracy_score
from sklearn.neural_network import MLPClassifier

# Vectorize the text data using TF-IDF
vectorizer = TfidfVectorizer(max_features=1000)
X_train_tfidf = vectorizer.fit_transform(X_train)
X_test_tfidf = vectorizer.transform(X_test)

# Create and train the MLPClassifier
nn_model = MLPClassifier(random_state=42, max_iter=300)
nn_model.fit(X_train_tfidf, y_train)

# Predict on the test data
nn_pred = nn_model.predict(X_test_tfidf)

# Accuracy scores
train_accuracy = nn_model.score(X_train_tfidf, y_train)
test_accuracy = accuracy_score(y_test, nn_pred)

# Evaluation
print("\n Neural Network Classification Report:")
print(classification_report(y_test, nn_pred))
print(f" Train Accuracy (Neural Network): {train_accuracy:.3f}")
print(f" Test Accuracy  (Neural Network): {test_accuracy:.3f}")

"""#   Visualize the performance metrics using confusion matrices, ROC curves, and precision-recall curves"""

import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix, roc_curve, precision_recall_curve, auc
# Function to plot confusion matrix
def plot_confusion_matrix(y_true, y_pred, classes, title='Confusion Matrix', cmap=plt.cm.Blues):
    cm = confusion_matrix(y_true, y_pred)
    plt.figure(figsize=(8, 6))
    plt.imshow(cm, interpolation='nearest', cmap=cmap)
    plt.title(title)
    plt.colorbar()
    tick_marks = np.arange(len(classes))
    plt.xticks(tick_marks, classes, rotation=45)
    plt.yticks(tick_marks, classes)
    plt.xlabel('Predicted Label')
    plt.ylabel('True Label')
    plt.tight_layout()
    for i in range(len(classes)):
        for j in range(len(classes)):
            plt.text(j, i, cm[i, j], horizontalalignment='center', color='white' if cm[i, j] > cm.max() / 2 else 'black')
    plt.show()
# Function to plot ROC curve
def plot_roc_curve(y_true, y_score):
    fpr, tpr, _ = roc_curve(y_true, y_score)
    roc_auc = auc(fpr, tpr)
    plt.figure()
    plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)
    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('Receiver Operating Characteristic (ROC) Curve')
    plt.legend(loc="lower right")
    plt.show()

# Function to plot precision-recall curve
def plot_precision_recall_curve(y_true, y_score):
    precision, recall, _ = precision_recall_curve(y_true, y_score)
    pr_auc = auc(recall, precision)
    plt.figure()
    plt.plot(recall, precision, color='blue', lw=2, label='Precision-Recall curve (area = %0.2f)' % pr_auc)
    plt.xlabel('Recall')
    plt.ylabel('Precision')
    plt.ylim([0.0, 1.05])
    plt.xlim([0.0, 1.0])
    plt.title('Precision-Recall Curve')
    plt.legend(loc="lower left")
    plt.show()

# Validate the selected model on the testing dataset
y_pred_probs = nn_model.predict(X_test_tfidf)
y_pred = (y_pred_probs > 0.5).astype(int)  # Apply threshold (0.5) to convert probabilities to binary predictions

# Plot confusion matrix
plot_confusion_matrix(y_test, y_pred, classes=['Non-Disaster', 'Disaster'])

# Plot ROC curve
plot_roc_curve(y_test, y_pred_probs)

# Plot precision-recall curve
plot_precision_recall_curve(y_test, y_pred_probs)

import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_recall_fscore_support
from tensorflow.keras.utils import plot_model

def compile_train_evaluate(model, model_name):
    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

    print(f"\n Training {model_name} Model...")
    history = model.fit(X_train_seq, y_train, epochs=5, batch_size=32, validation_split=0.2, verbose=1)

    # Evaluate on Test Set
    print(f"\n Evaluating {model_name}...")
    test_loss, test_accuracy = model.evaluate(X_test_seq, y_test, verbose=0)
    train_accuracy = model.evaluate(X_train_seq, y_train, verbose=0)[1]

    print(f" Train Accuracy: {train_accuracy:.4f}")
    print(f" Test Accuracy : {test_accuracy:.4f}")

    # Predictions
    y_pred = (model.predict(X_test_seq) > 0.5).astype("int32")

    # Classification Report
    print("\n Classification Report:")
    report = classification_report(y_test, y_pred, digits=4)
    print(report)

    # Precision, Recall, F1, Support
    precision, recall, f1, support = precision_recall_fscore_support(y_test, y_pred, average='binary')
    print(f"\n Precision: {precision:.4f} | Recall: {recall:.4f} | F1 Score: {f1:.4f} | Support: {support}")

    # Confusion Matrix
    cm = confusion_matrix(y_test, y_pred)
    plt.figure(figsize=(6, 5))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
    plt.title(f'{model_name} Confusion Matrix')
    plt.xlabel('Predicted')
    plt.ylabel('True')
    plt.show()

    # Training History Plots
    plt.figure(figsize=(12, 4))

    # Accuracy Plot
    plt.subplot(1, 2, 1)
    plt.plot(history.history['accuracy'], label='Train Acc')
    plt.plot(history.history['val_accuracy'], label='Val Acc')
    plt.axhline(y=test_accuracy, color='red', linestyle='--', label='Test Acc')
    plt.title(f'{model_name} Accuracy')
    plt.legend()

    # Loss Plot
    plt.subplot(1, 2, 2)
    plt.plot(history.history['loss'], label='Train Loss')
    plt.plot(history.history['val_loss'], label='Val Loss')
    plt.title(f'{model_name} Loss')
    plt.legend()

    plt.show()

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, GRU, Conv1D, MaxPooling1D, GlobalMaxPooling1D, Dense, Dropout

def create_lstm_model():
    model = Sequential([
        Embedding(input_dim=vocab_size, output_dim=100, input_length=sequence_length),
        LSTM(64),
        Dense(1, activation='sigmoid')
    ])
    return model

def create_cnn_model():
    model = Sequential([
        Embedding(input_dim=vocab_size, output_dim=100, input_length=sequence_length),
        Conv1D(128, 5, activation='relu'),
        MaxPooling1D(pool_size=2),
        GlobalMaxPooling1D(),
        Dense(1, activation='sigmoid')
    ])
    return model

def create_gru_model():
    model = Sequential([
        Embedding(input_dim=vocab_size, output_dim=100, input_length=sequence_length),
        GRU(64),
        Dense(1, activation='sigmoid')
    ])
    return model

from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

# Tokenize text
tokenizer = Tokenizer(num_words=10000, oov_token='<OOV>')
tokenizer.fit_on_texts(X_train)

# Define sequence length
sequence_length = 100 # You can adjust this based on your text analysis

X_train_seq = pad_sequences(tokenizer.texts_to_sequences(X_train), maxlen=sequence_length)
X_test_seq = pad_sequences(tokenizer.texts_to_sequences(X_test), maxlen=sequence_length)

vocab_size = len(tokenizer.word_index) + 1

# LSTM
lstm_model = create_lstm_model()
compile_train_evaluate(lstm_model, "LSTM")

# CNN
cnn_model = create_cnn_model()
compile_train_evaluate(cnn_model, "CNN")

# GRU (Optional)
gru_model = create_gru_model()
compile_train_evaluate(gru_model, "GRU")

"""##  Insights from Model Evaluation

1. **Logistic Regression**

   * Strong baseline model with balanced precision and recall for both classes.
   * Achieves **80% test accuracy** with good F1-scores (0.83 for class 0, 0.74 for class 1).
   * Slightly better recall for class 0 (0.89) than class 1 (0.67), indicating more false negatives in class 1.

2. **Random Forest**

   * Very high training accuracy (98.6%) indicating possible overfitting.
   * Test accuracy drops to 77.9%, still competitive but slightly worse than logistic regression.
   * Precision and recall show imbalance with better performance on class 0 than class 1, consistent with Logistic Regression.

3. **Neural Network (MLPClassifier)**

   * High training accuracy (\~98%), again risk of overfitting.
   * Test accuracy at 76.6%, slightly below Random Forest and Logistic Regression.
   * Precision and recall are more balanced but generally lower than Logistic Regression.

4. **LSTM**

   * Good train accuracy (93.7%) with slightly lower test accuracy (76.3%).
   * Balanced precision and recall (\~0.79 and 0.73 for class 0 and 1 respectively), showing ability to model sequential text features.
   * F1 scores around 0.80 for class 0 and 0.72 for class 1.

5. **CNN**

   * Highest test accuracy among deep models (78.4%), close to Logistic Regression.
   * Balanced precision and recall for both classes with F1-scores 0.82 (class 0) and 0.73 (class 1).
   * Strong generalization and ability to extract local features in text.

6. **GRU**

   * Similar training accuracy (\~93.7%) and slightly lower test accuracy (75.3%) compared to LSTM and CNN.
   * Precision and recall are balanced but slightly lower than CNN and LSTM, with F1-scores around 0.78 (class 0) and 0.72 (class 1).

---

##  Summary Table of Model Performance

| Model               | Train Accuracy | Test Accuracy | Precision (0) | Recall (0) | F1-Score (0) | Precision (1) | Recall (1) | F1-Score (1) | Macro F1 | Weighted F1 |
| ------------------- | -------------- | ------------- | ------------- | ---------- | ------------ | ------------- | ---------- | ------------ | -------- | ----------- |
| Logistic Regression | 0.852          | 0.796         | 0.78          | 0.89       | 0.83         | 0.82          | 0.67       | 0.74         | 0.78     | 0.79        |
| Random Forest       | 0.986          | 0.779         | 0.77          | 0.88       | 0.82         | 0.80          | 0.64       | 0.71         | 0.77     | 0.77        |
| Neural Network      | 0.981          | 0.766         | 0.79          | 0.80       | 0.80         | 0.73          | 0.72       | 0.72         | 0.76     | 0.77        |
| LSTM                | 0.937          | 0.763         | 0.79          | 0.80       | 0.80         | 0.73          | 0.71       | 0.72         | 0.76     | 0.76        |
| CNN                 | 0.945          | 0.784         | 0.79          | 0.86       | 0.82         | 0.78          | 0.68       | 0.73         | 0.77     | 0.78        |
| GRU                 | 0.937          | 0.753         | 0.80          | 0.76       | 0.78         | 0.70          | 0.74       | 0.72         | 0.75     | 0.75        |

---

##  Key Takeaways

* **Logistic Regression and CNN models perform best** on test data, balancing precision and recall effectively.
* Random Forest shows signs of **overfitting** (high train accuracy, lower test accuracy).
* Deep learning models (LSTM, CNN, GRU) capture sequential/text features well but require tuning to surpass simpler models.
* Class 1 (minority or positive class) generally has lower recall and F1, indicating room for improvement in identifying positives.


"""

import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
import pandas as pd

# Sample data: Create a correlation matrix
np.random.seed(42)
data = np.random.rand(10, 10)
df = pd.DataFrame(data, columns=[f"Feature {i}" for i in range(1, 11)])

# Compute the correlation matrix (if applicable)
corr_matrix = df.corr()

# Create the heatmap
plt.figure(figsize=(10, 8))
sns.heatmap(
    corr_matrix,
    annot=True,
    cmap='coolwarm',
    fmt=".2f",
    linewidths=0.5,
    linecolor='white',
    vmin=-1, vmax=1,
    cbar_kws={'label': 'Correlation'},
    mask=np.triu(corr_matrix)
    square=True,
    xticklabels=df.columns,
    yticklabels=df.columns
)

# Title for the plot
plt.title("Updated Heatmap with Annotations and Customization", fontsize=16)

# Display the plot
plt.tight_layout()
plt.show()

"""# Handling Overfitting and saving this file to pickle and mlp:"""

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, Dense, Dropout, LSTM
from tensorflow.keras.callbacks import EarlyStopping

model = Sequential([
    Embedding(input_dim=vocab_size, output_dim=100, input_length=sequence_length),
    LSTM(64, return_sequences=False),
    Dropout(0.5),
    Dense(1, activation='sigmoid')
])

model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

early_stop = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)

model.fit(X_train_seq, y_train, epochs=10, batch_size=32,
          validation_split=0.2, callbacks=[early_stop])

import numpy as np
import pandas as pd
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.neural_network import MLPClassifier
from sklearn.metrics import classification_report
from sklearn.utils.class_weight import compute_class_weight
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
from tensorflow.keras.regularizers import l2
import warnings
warnings.filterwarnings('ignore')
import pickle
from IPython.display import FileLink

# Generate synthetic data
X, y = make_classification(n_samples=1000, n_features=20, n_informative=15,
                           n_redundant=5, n_classes=2, weights=[0.7, 0.3], random_state=42)

# Define ModelImprover
class ModelImprover:
    def __init__(self, X, y, test_size=0.2, random_state=42):
        self.X = X
        self.y = y
        self.test_size = test_size
        self.random_state = random_state
        self.scaler = StandardScaler()
        self.models = {} # Initialize an empty dictionary to store models

    def prepare_data(self):
        X_train, X_test, y_train, y_test = train_test_split(
            self.X, self.y, test_size=self.test_size,
            random_state=self.random_state, stratify=self.y
        )
        X_train_scaled = self.scaler.fit_transform(X_train)
        X_test_scaled = self.scaler.transform(X_test)
        return X_train_scaled, X_test_scaled, y_train, y_test

    def improved_logistic_regression(self, X_train, X_test, y_train, y_test):
        class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)
        class_weight_dict = {i: class_weights[i] for i in range(len(class_weights))}

        param_grid = {
            'C': [0.01, 0.1, 1, 10],
            'penalty': ['l2'],
            'solver': ['liblinear']
        }

        lr = LogisticRegression(class_weight=class_weight_dict, max_iter=1000)
        grid_search = GridSearchCV(lr, param_grid, cv=3, scoring='f1_macro', n_jobs=-1)
        grid_search.fit(X_train, y_train)

        best_lr = grid_search.best_estimator_
        train_acc = best_lr.score(X_train, y_train)
        test_acc = best_lr.score(X_test, y_test)

        print(" Logistic Regression")
        print("Best Parameters:", grid_search.best_params_)
        print(f"Train Accuracy: {train_acc:.3f}, Test Accuracy: {test_acc:.3f}")
        print(classification_report(y_test, best_lr.predict(X_test)))
        print("-" * 50)
        return best_lr

    def improved_random_forest(self, X_train, X_test, y_train, y_test):
        class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)
        class_weight_dict = {i: class_weights[i] for i in range(len(class_weights))}

        param_grid = {
            'n_estimators': [100],
            'max_depth': [5, 10, None],
            'min_samples_split': [2, 5],
        }

        rf = RandomForestClassifier(class_weight=class_weight_dict, random_state=self.random_state)
        grid_search = GridSearchCV(rf, param_grid, cv=3, scoring='f1_macro', n_jobs=-1)
        grid_search.fit(X_train, y_train)

        best_rf = grid_search.best_estimator_
        train_acc = best_rf.score(X_train, y_train)
        test_acc = best_rf.score(X_test, y_test)

        print(" Random Forest")
        print("Best Parameters:", grid_search.best_params_)
        print(f"Train Accuracy: {train_acc:.3f}, Test Accuracy: {test_acc:.3f}")
        print(classification_report(y_test, best_rf.predict(X_test)))
        print("-" * 50)
        return best_rf

    def improved_neural_network(self, X_train, X_test, y_train, y_test):
        class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)
        class_weight_dict = {i: class_weights[i] for i in range(len(class_weights))}

        param_grid = {
            'hidden_layer_sizes': [(50, 50)],
            'alpha': [0.001],
            'learning_rate': ['adaptive']
        }

        mlp = MLPClassifier(random_state=self.random_state, max_iter=1000)
        grid_search = GridSearchCV(mlp, param_grid, cv=3, scoring='f1_macro', n_jobs=-1)
        grid_search.fit(X_train, y_train)

        best_mlp = grid_search.best_estimator_
        train_acc = best_mlp.score(X_train, y_train)
        test_acc = best_mlp.score(X_test, y_test)

        print(" Neural Network (MLPClassifier)")
        print("Best Parameters:", grid_search.best_params_)
        print(f"Train Accuracy: {train_acc:.3f}, Test Accuracy: {test_acc:.3f}")
        print(classification_report(y_test, best_mlp.predict(X_test)))
        print("-" * 50)
        return best_mlp

    def improved_dense_nn(self, X_train, X_test, y_train, y_test):
        class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)
        class_weight_dict = {i: class_weights[i] for i in range(len(class_weights))}

        model = Sequential([
            Dense(64, activation='relu', kernel_regularizer=l2(0.01), input_shape=(X_train.shape[1],)),
            Dropout(0.3),
            Dense(32, activation='relu', kernel_regularizer=l2(0.01)),
            Dropout(0.3),
            Dense(1, activation='sigmoid')
        ])

        model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
        early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)
        reduce_lr = ReduceLROnPlateau(monitor='val_loss', patience=3, factor=0.5, min_lr=1e-5)

        model.fit(X_train, y_train, epochs=100, batch_size=32, validation_split=0.2,
                  callbacks=[early_stop, reduce_lr], class_weight=class_weight_dict, verbose=0)

        train_acc = model.evaluate(X_train, y_train, verbose=0)[1]
        test_acc = model.evaluate(X_test, y_test, verbose=0)[1]

        print(" Keras Dense Neural Network")
        print(f"Train Accuracy: {train_acc:.3f}, Test Accuracy: {test_acc:.3f}")
        y_pred = (model.predict(X_test) > 0.5).astype(int)
        print(classification_report(y_test, y_pred))
        print("-" * 50)
        return model


    def run_all(self):
        print("=" * 60)
        print("Running All Improved Models")
        print("=" * 60)
        X_train, X_test, y_train, y_test = self.prepare_data()
        print(f"Train size: {X_train.shape[0]}, Test size: {X_test.shape[0]}")
        print("-" * 50)

        self.models['Logistic Regression'] = self.improved_logistic_regression(X_train, X_test, y_train, y_test)
        self.models['Random Forest'] = self.improved_random_forest(X_train, X_test, y_train, y_test)
        self.models['MLP Classifier'] = self.improved_neural_network(X_train, X_test, y_train, y_test)
        self.models['Dense NN (Keras)'] = self.improved_dense_nn(X_train, X_test, y_train, y_test)

        # No need to return models here, they are stored in self.models

    def get_model(self, model_name):
        """Retrieves a trained model by name."""
        return self.models.get(model_name)


# Run it
improver = ModelImprover(X, y)
improver.run_all() # Call run_all to train models and store them in improver.models

# Extract the mlp model and scaler from the improver object
mlp = improver.get_model('MLP Classifier')
scaler = improver.scaler
with open('mlp_model.pkl', 'wb') as f:
    pickle.dump(mlp, f)

with open('scaler.pkl', 'wb') as f:
    pickle.dump(scaler, f)

print(" Model and scaler saved!")

"""## **Summary of Model Performance**

| Model                    | Train Accuracy | Test Accuracy | Overfitting Gap | F1 Score (Class 1) |
| ------------------------ | -------------- | ------------- | --------------- | ------------------ |
| Logistic Regression      | 0.800          | 0.840         | -0.040          | 0.75               |
| Random Forest            | 1.000          | 0.910         | 0.090           | 0.84               |
| MLP Classifier (Sklearn) | 1.000          | 0.975         | 0.025           | 0.96               |
| Dense NN (Keras)         | 0.976          | 0.925         | 0.051           | 0.88               |

---

##  **Model-wise Insights**

### 1. **Logistic Regression**

* **Test Accuracy:** 84% — very reasonable.
* **F1 Score (Class 1):** 0.75 — indicates balanced performance but slightly lower recall.
* **Insight:** A good baseline model with **low overfitting** and consistent performance. Best for interpretability.

---

### 2. **Random Forest**

* **Train Accuracy:** 100%, Test Accuracy: 91% — moderate overfitting.
* **F1 Score (Class 1):** 0.84 — strong performance, especially in precision.
* **Insight:** Handles imbalance well, but **may be overfitting** slightly. Can benefit from regularization (e.g., max depth, limiting features).

---

### 3. **MLP Classifier (Sklearn)**

* **Test Accuracy:** 97.5% — highest among all.
* **F1 Score (Class 1):** 0.96 — excellent balance of precision and recall.
* **Insight:** Outstanding performance with minimal overfitting (gap = 2.5%). This model currently performs **best overall**.

---

### 4. **Dense Neural Network (Keras)**

* **Test Accuracy:** 92.5%, F1 (Class 1): 0.88
* **Insight:** Very strong performance, especially for the minority class (class 1 recall = 0.93).
* **Good generalization**, thanks to dropout and L2 regularization. A good candidate for deep learning-based deployment.
"""









